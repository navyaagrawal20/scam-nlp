{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8cY8h66Sh3K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM2KMUu0SxIY",
        "outputId": "6a1868c3-e82a-49ff-fc21-f58a41092288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Unnamed: 0', 'label', 'text', 'label_num'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/spam_ham_dataset.csv\", encoding='latin-1')\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KmQWJnSES0sy",
        "outputId": "787558ee-d912-4982-8ce7-ca547a1d0c98"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5171,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1492,\n        \"min\": 0,\n        \"max\": 5170,\n        \"num_unique_values\": 5171,\n        \"samples\": [\n          2924,\n          3839,\n          3078\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4993,\n        \"samples\": [\n          \"Subject: hpl / conoco - teco waha 03 / 23 / 01 purchase\\r\\ndaren , conoco invoiced hpl at $ 5 . 87 for 03 / 23 at pgev / waha and deal ticket 685350 shows $ 4 . 87 . can you confirm the price ? thanks .\",\n          \"Subject: holiday on - call data\\r\\npipeline contact phone fax pager\\r\\nblack marlin blair lichentwalter 713 853 - 7367 713 646 - 3201 ( h )\\r\\n281 370 - 1866\\r\\ndebbie thompson 713 853 - 3144 713 646 - 3201\\r\\n( noms due today for 23 rd through 27 th )\\r\\nchannel jim tobacco 713 420 - 2159\\r\\ngas control 1 505 599 - 2333\\r\\n( open thursday . noms will be due through monday )\\r\\ncentana william spekels 713 627 - 6290 713 762 - 3450\\r\\ndonna spencer 713 627 - 6255\\r\\ngas control 1 888 204 - 1718\\r\\n( noms due today for 23 rd through 27 th )\\r\\nduke energy annette anderson 713 260 - 8603 713 949 - 3026\\r\\n( on call ) bob moseman 713 - 260 - 8698 ( thursday )\\r\\nopen tomorrow - noms will be due thru the 27 th )\\r\\nlonestar gary gafford 214 670 - 2674 214 875 - 3810\\r\\ngas control 214 875 - 2455 or 2456\\r\\n( noms due today , 23 rd thru 27 th )\\r\\nnorthern natural ben markey 853 - 7581 cell 713 446 - 9404 800 931 - 0398\\r\\n( on call ) charlie mosey 853 - 1520\\r\\ngas control 853 -\\r\\n( open thursday - noms due thru 27 th . )\\r\\neast trans - east texas\\r\\ntejas gas control 713 767 - 5366\\r\\npaula svehla 713 230 - 3569\\r\\nmickey chapman 713 230 - 3546\\r\\n( open thursday - noms due thru 27 th )\\r\\nmidcon ( y 2 k ) ken nachlinger 713 369 - 9284 713 369 - 9375 888 733 - 5954\\r\\n( on call ) steven 888 790 - 0255\\r\\n( y 2 k ) don 888 733 - 4602\\r\\ngas control 713 369 - 9200\\r\\n( noms due today , 23 rd thru 27 th )\\r\\nmoss bluff no current business\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-bcf6bf6c-ee6f-4631-ae08-cedf4c98b351\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bcf6bf6c-ee6f-4631-ae08-cedf4c98b351')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bcf6bf6c-ee6f-4631-ae08-cedf4c98b351 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bcf6bf6c-ee6f-4631-ae08-cedf4c98b351');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2d3cc2e7-8282-42da-b172-04a458ff0b43\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2d3cc2e7-8282-42da-b172-04a458ff0b43')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2d3cc2e7-8282-42da-b172-04a458ff0b43 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   Unnamed: 0 label                                               text  \\\n",
              "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
              "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
              "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
              "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
              "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
              "\n",
              "   label_num  \n",
              "0          0  \n",
              "1          0  \n",
              "2          0  \n",
              "3          1  \n",
              "4          0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlJTYAYFTOOs",
        "outputId": "4b18d7e8-2d5e-4f48-c47d-730b33a17c56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5171, 4)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFmXdNg8TiWA",
        "outputId": "60c73b25-6834-4394-abf5-494c83821f23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  \\\n",
            "0  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
            "1  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
            "2  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
            "3  Subject: photoshop , windows , office . cheap ...   \n",
            "4  Subject: re : indian springs\\r\\nthis deal is t...   \n",
            "\n",
            "                                       string_tokens  \\\n",
            "0  [Subject:, enron, methanol, ;, meter, #, :, 98...   \n",
            "1  [Subject:, hpl, nom, for, january, 9, ,, 2001,...   \n",
            "2  [Subject:, neon, retreat, ho, ho, ho, ,, we, '...   \n",
            "3  [Subject:, photoshop, ,, windows, ,, office, ....   \n",
            "4  [Subject:, re, :, indian, springs, this, deal,...   \n",
            "\n",
            "                                    nltk_word_tokens  \\\n",
            "0  [Subject, :, enron, methanol, ;, meter, #, :, ...   \n",
            "1  [Subject, :, hpl, nom, for, january, 9, ,, 200...   \n",
            "2  [Subject, :, neon, retreat, ho, ho, ho, ,, we,...   \n",
            "3  [Subject, :, photoshop, ,, windows, ,, office,...   \n",
            "4  [Subject, :, re, :, indian, springs, this, dea...   \n",
            "\n",
            "                                nltk_sentence_tokens  \n",
            "0  [Subject: enron methanol ; meter # : 988291\\r\\...  \n",
            "1  [Subject: hpl nom for january 9 , 2001\\r\\n( se...  \n",
            "2  [Subject: neon retreat\\r\\nho ho ho , we ' re a...  \n",
            "3  [Subject: photoshop , windows , office ., chea...  \n",
            "4  [Subject: re : indian springs\\r\\nthis deal is ...  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')  # Make sure punkt is downloaded\n",
        "\n",
        "# Assuming you have a DataFrame `df` with a column `text`\n",
        "df['string_tokens'] = df['text'].apply(lambda x: x.split())  # Simple space-based tokenization\n",
        "df['nltk_word_tokens'] = df['text'].apply(lambda x: word_tokenize(str(x)))  # Word tokenization using NLTK\n",
        "df['nltk_sentence_tokens'] = df['text'].apply(lambda x: sent_tokenize(str(x)))  # Sentence tokenization using NLTK\n",
        "\n",
        "# Display the results\n",
        "print(df[['text', 'string_tokens', 'nltk_word_tokens', 'nltk_sentence_tokens']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csiklIM6Tv4Z",
        "outputId": "04bdf71e-11f6-4e4b-93dc-c6c3b6c1e4b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most frequent word tokens:\n",
            "-: 85723\n",
            ".: 54681\n",
            "/: 42848\n",
            ",: 40640\n",
            ":: 30275\n",
            "the: 25613\n",
            "to: 20332\n",
            "ect: 13900\n",
            "and: 12815\n",
            "@: 12735\n",
            "\n",
            "---\n",
            "\n",
            "Most frequent sentence tokens:\n",
            ".: 4439\n",
            "?: 2111\n",
            "!: 817\n",
            "xls: 482\n",
            "thanks .: 316\n",
            "s .: 213\n",
            "computron - me .: 155\n",
            "63 .: 142\n",
            "161 .: 139\n",
            "doc: 127\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Flatten the list of word tokens\n",
        "all_word_tokens = [token for sublist in df['nltk_word_tokens'] for token in sublist]\n",
        "\n",
        "# Get the most frequent word tokens\n",
        "word_token_counts = Counter(all_word_tokens)\n",
        "most_common_words = word_token_counts.most_common(10) # Get top 10 most common words\n",
        "\n",
        "print(\"Most frequent word tokens:\")\n",
        "for word, count in most_common_words:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(\"\\n---\\n\")\n",
        "\n",
        "# Flatten the list of sentence tokens\n",
        "all_sentence_tokens = [token for sublist in df['nltk_sentence_tokens'] for token in sublist]\n",
        "\n",
        "# Get the most frequent sentence tokens\n",
        "sentence_token_counts = Counter(all_sentence_tokens)\n",
        "most_common_sentences = sentence_token_counts.most_common(10) # Get top 10 most common sentences\n",
        "\n",
        "print(\"Most frequent sentence tokens:\")\n",
        "for sentence, count in most_common_sentences:\n",
        "    print(f\"{sentence}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91T5zb-aVn1c",
        "outputId": "7f217f3d-6a59-493c-93a9-4509853766c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  \\\n",
            "0  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
            "1  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
            "2  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
            "3  Subject: photoshop , windows , office . cheap ...   \n",
            "4  Subject: re : indian springs\\r\\nthis deal is t...   \n",
            "\n",
            "                                    nltk_word_tokens  \\\n",
            "0  [Subject, :, enron, methanol, ;, meter, #, :, ...   \n",
            "1  [Subject, :, hpl, nom, for, january, 9, ,, 200...   \n",
            "2  [Subject, :, neon, retreat, ho, ho, ho, ,, we,...   \n",
            "3  [Subject, :, photoshop, ,, windows, ,, office,...   \n",
            "4  [Subject, :, re, :, indian, springs, this, dea...   \n",
            "\n",
            "                            cleaned_nltk_word_tokens  \n",
            "0  [subject, enron, methanol, meter, 988291, foll...  \n",
            "1  [subject, hpl, nom, january, 9, 2001, see, att...  \n",
            "2  [subject, neon, retreat, ho, ho, ho, around, w...  \n",
            "3  [subject, photoshop, windows, office, cheap, m...  \n",
            "4  [subject, indian, springs, deal, book, teco, p...  \n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string # Import the string module\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get the list of English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a function to preprocess tokens with recursive flattening and cleaning\n",
        "def preprocess(tokens):\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    def process_item(item):\n",
        "        if isinstance(item, list):\n",
        "            for sub_item in item:\n",
        "                process_item(sub_item)\n",
        "        elif isinstance(item, str):\n",
        "            t_lower = item.lower()\n",
        "            if t_lower not in string.punctuation and t_lower not in stop_words:\n",
        "                cleaned_tokens.append(t_lower)\n",
        "        # Optionally handle other types if necessary\n",
        "        # else:\n",
        "        #     print(f\"Warning: Skipping non-list, non-string element: {item}\")\n",
        "\n",
        "    # Start processing from the top-level input\n",
        "    if isinstance(tokens, list):\n",
        "        process_item(tokens)\n",
        "    # Optionally handle non-list top-level input if necessary\n",
        "    # elif isinstance(tokens, str):\n",
        "    #     process_item(tokens)\n",
        "\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "\n",
        "# Assuming 'nltk_word_tokens' is a column in your DataFrame containing lists of word tokens\n",
        "# If not, you'll need to create it first by tokenizing the 'text' column\n",
        "# For example: df['nltk_word_tokens'] = df['text'].apply(lambda x: word_tokenize(str(x)))\n",
        "\n",
        "\n",
        "# Apply preprocessing to the 'nltk_word_tokens' column\n",
        "df['cleaned_nltk_word_tokens'] = df['nltk_word_tokens'].apply(preprocess)\n",
        "\n",
        "\n",
        "# Display the updated DataFrame with cleaned tokens\n",
        "print(df[['text', 'nltk_word_tokens', 'cleaned_nltk_word_tokens']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvbhrL_pyBbW"
      },
      "source": [
        "# Expt 3 - Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBGPSb2eV54q",
        "outputId": "efb00300-dfd6-465a-8ef5-2ce7c797cb6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize stemmer & lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3401f5ac",
        "outputId": "b20830d4-296b-47d3-b036-274ff8ab6718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Sample before & after stemming:\n",
            "\n",
            "Original : ['subject', 'enron', 'methanol', 'meter', '988291', 'follow', 'note', 'gave', 'monday', '4', '3', '00', 'preliminary', 'flow', 'data', 'provided', 'daren', 'please', 'override', 'pop', 'daily', 'volume', 'presently', 'zero', 'reflect', 'daily', 'activity', 'obtain', 'gas', 'control', 'change', 'needed', 'asap', 'economics', 'purposes']\n",
            "Stemmed  : ['subject', 'enron', 'methanol', 'meter', '988291', 'follow', 'note', 'gave', 'monday', '4', '3', '00', 'preliminari', 'flow', 'data', 'provid', 'daren', 'pleas', 'overrid', 'pop', 'daili', 'volum', 'present', 'zero', 'reflect', 'daili', 'activ', 'obtain', 'ga', 'control', 'chang', 'need', 'asap', 'econom', 'purpos']\n",
            "--------------------------------------------------\n",
            "Original : ['subject', 'hpl', 'nom', 'january', '9', '2001', 'see', 'attached', 'file', 'hplnol', '09', 'xls', 'hplnol', '09', 'xls']\n",
            "Stemmed  : ['subject', 'hpl', 'nom', 'januari', '9', '2001', 'see', 'attach', 'file', 'hplnol', '09', 'xl', 'hplnol', '09', 'xl']\n",
            "--------------------------------------------------\n",
            "Original : ['subject', 'neon', 'retreat', 'ho', 'ho', 'ho', 'around', 'wonderful', 'time', 'year', 'neon', 'leaders', 'retreat', 'time', 'know', 'time', 'year', 'extremely', 'hectic', 'tough', 'think', 'anything', 'past', 'holidays', 'life', 'go', 'past', 'week', 'december', '25', 'january', '1', 'like', 'think', 'minute', 'calender', 'handed', 'beginning', 'fall', 'semester', 'retreat', 'scheduled', 'weekend', 'january', '5', '6', 'youth', 'ministers', 'conference', 'brad', 'dustin', 'connected', 'week', 'going', 'change', 'date', 'following', 'weekend', 'january', '12', '13', 'comes', 'part', 'need', 'think', 'think', 'agree', 'important', 'us', 'get', 'together', 'time', 'recharge', 'batteries', 'get', 'far', 'spring', 'semester', 'lot', 'trouble', 'difficult', 'us', 'get', 'away', 'without', 'kids', 'etc', 'brad', 'came', 'potential', 'alternative', 'get', 'together', 'weekend', 'let', 'know', 'prefer', 'first', 'option', 'would', 'retreat', 'similar', 'done', 'past', 'several', 'years', 'year', 'could', 'go', 'heartland', 'country', 'inn', 'www', 'com', 'outside', 'brenham', 'nice', 'place', '13', 'bedroom', '5', 'bedroom', 'house', 'side', 'side', 'country', 'real', 'relaxing', 'also', 'close', 'brenham', 'one', 'hour', '15', 'minutes', 'golf', 'shop', 'antique', 'craft', 'stores', 'brenham', 'eat', 'dinner', 'together', 'ranch', 'spend', 'time', 'meet', 'saturday', 'return', 'sunday', 'morning', 'like', 'done', 'past', 'second', 'option', 'would', 'stay', 'houston', 'dinner', 'together', 'nice', 'restaurant', 'dessert', 'time', 'visiting', 'recharging', 'one', 'homes', 'saturday', 'evening', 'might', 'easier', 'trade', 'would', 'much', 'time', 'together', 'let', 'decide', 'email', 'back', 'would', 'preference', 'course', 'available', 'weekend', 'democratic', 'process', 'prevail', 'majority', 'vote', 'rule', 'let', 'hear', 'soon', 'possible', 'preferably', 'end', 'weekend', 'vote', 'go', 'way', 'complaining', 'allowed', 'like', 'tend', 'great', 'weekend', 'great', 'golf', 'great', 'fishing', 'great', 'shopping', 'whatever', 'makes', 'happy', 'bobby']\n",
            "Stemmed  : ['subject', 'neon', 'retreat', 'ho', 'ho', 'ho', 'around', 'wonder', 'time', 'year', 'neon', 'leader', 'retreat', 'time', 'know', 'time', 'year', 'extrem', 'hectic', 'tough', 'think', 'anyth', 'past', 'holiday', 'life', 'go', 'past', 'week', 'decemb', '25', 'januari', '1', 'like', 'think', 'minut', 'calend', 'hand', 'begin', 'fall', 'semest', 'retreat', 'schedul', 'weekend', 'januari', '5', '6', 'youth', 'minist', 'confer', 'brad', 'dustin', 'connect', 'week', 'go', 'chang', 'date', 'follow', 'weekend', 'januari', '12', '13', 'come', 'part', 'need', 'think', 'think', 'agre', 'import', 'us', 'get', 'togeth', 'time', 'recharg', 'batteri', 'get', 'far', 'spring', 'semest', 'lot', 'troubl', 'difficult', 'us', 'get', 'away', 'without', 'kid', 'etc', 'brad', 'came', 'potenti', 'altern', 'get', 'togeth', 'weekend', 'let', 'know', 'prefer', 'first', 'option', 'would', 'retreat', 'similar', 'done', 'past', 'sever', 'year', 'year', 'could', 'go', 'heartland', 'countri', 'inn', 'www', 'com', 'outsid', 'brenham', 'nice', 'place', '13', 'bedroom', '5', 'bedroom', 'hous', 'side', 'side', 'countri', 'real', 'relax', 'also', 'close', 'brenham', 'one', 'hour', '15', 'minut', 'golf', 'shop', 'antiqu', 'craft', 'store', 'brenham', 'eat', 'dinner', 'togeth', 'ranch', 'spend', 'time', 'meet', 'saturday', 'return', 'sunday', 'morn', 'like', 'done', 'past', 'second', 'option', 'would', 'stay', 'houston', 'dinner', 'togeth', 'nice', 'restaur', 'dessert', 'time', 'visit', 'recharg', 'one', 'home', 'saturday', 'even', 'might', 'easier', 'trade', 'would', 'much', 'time', 'togeth', 'let', 'decid', 'email', 'back', 'would', 'prefer', 'cours', 'avail', 'weekend', 'democrat', 'process', 'prevail', 'major', 'vote', 'rule', 'let', 'hear', 'soon', 'possibl', 'prefer', 'end', 'weekend', 'vote', 'go', 'way', 'complain', 'allow', 'like', 'tend', 'great', 'weekend', 'great', 'golf', 'great', 'fish', 'great', 'shop', 'whatev', 'make', 'happi', 'bobbi']\n",
            "--------------------------------------------------\n",
            "üîπ Sample before & after lemmatization:\n",
            "\n",
            "Original    : ['subject', 'enron', 'methanol', 'meter', '988291', 'follow', 'note', 'gave', 'monday', '4', '3', '00', 'preliminary', 'flow', 'data', 'provided', 'daren', 'please', 'override', 'pop', 'daily', 'volume', 'presently', 'zero', 'reflect', 'daily', 'activity', 'obtain', 'gas', 'control', 'change', 'needed', 'asap', 'economics', 'purposes']\n",
            "Lemmatized  : ['subject', 'enron', 'methanol', 'meter', '988291', 'follow', 'note', 'gave', 'monday', '4', '3', '00', 'preliminary', 'flow', 'data', 'provided', 'daren', 'please', 'override', 'pop', 'daily', 'volume', 'presently', 'zero', 'reflect', 'daily', 'activity', 'obtain', 'gas', 'control', 'change', 'needed', 'asap', 'economics', 'purpose']\n",
            "--------------------------------------------------\n",
            "Original    : ['subject', 'hpl', 'nom', 'january', '9', '2001', 'see', 'attached', 'file', 'hplnol', '09', 'xls', 'hplnol', '09', 'xls']\n",
            "Lemmatized  : ['subject', 'hpl', 'nom', 'january', '9', '2001', 'see', 'attached', 'file', 'hplnol', '09', 'xl', 'hplnol', '09', 'xl']\n",
            "--------------------------------------------------\n",
            "Original    : ['subject', 'neon', 'retreat', 'ho', 'ho', 'ho', 'around', 'wonderful', 'time', 'year', 'neon', 'leaders', 'retreat', 'time', 'know', 'time', 'year', 'extremely', 'hectic', 'tough', 'think', 'anything', 'past', 'holidays', 'life', 'go', 'past', 'week', 'december', '25', 'january', '1', 'like', 'think', 'minute', 'calender', 'handed', 'beginning', 'fall', 'semester', 'retreat', 'scheduled', 'weekend', 'january', '5', '6', 'youth', 'ministers', 'conference', 'brad', 'dustin', 'connected', 'week', 'going', 'change', 'date', 'following', 'weekend', 'january', '12', '13', 'comes', 'part', 'need', 'think', 'think', 'agree', 'important', 'us', 'get', 'together', 'time', 'recharge', 'batteries', 'get', 'far', 'spring', 'semester', 'lot', 'trouble', 'difficult', 'us', 'get', 'away', 'without', 'kids', 'etc', 'brad', 'came', 'potential', 'alternative', 'get', 'together', 'weekend', 'let', 'know', 'prefer', 'first', 'option', 'would', 'retreat', 'similar', 'done', 'past', 'several', 'years', 'year', 'could', 'go', 'heartland', 'country', 'inn', 'www', 'com', 'outside', 'brenham', 'nice', 'place', '13', 'bedroom', '5', 'bedroom', 'house', 'side', 'side', 'country', 'real', 'relaxing', 'also', 'close', 'brenham', 'one', 'hour', '15', 'minutes', 'golf', 'shop', 'antique', 'craft', 'stores', 'brenham', 'eat', 'dinner', 'together', 'ranch', 'spend', 'time', 'meet', 'saturday', 'return', 'sunday', 'morning', 'like', 'done', 'past', 'second', 'option', 'would', 'stay', 'houston', 'dinner', 'together', 'nice', 'restaurant', 'dessert', 'time', 'visiting', 'recharging', 'one', 'homes', 'saturday', 'evening', 'might', 'easier', 'trade', 'would', 'much', 'time', 'together', 'let', 'decide', 'email', 'back', 'would', 'preference', 'course', 'available', 'weekend', 'democratic', 'process', 'prevail', 'majority', 'vote', 'rule', 'let', 'hear', 'soon', 'possible', 'preferably', 'end', 'weekend', 'vote', 'go', 'way', 'complaining', 'allowed', 'like', 'tend', 'great', 'weekend', 'great', 'golf', 'great', 'fishing', 'great', 'shopping', 'whatever', 'makes', 'happy', 'bobby']\n",
            "Lemmatized  : ['subject', 'neon', 'retreat', 'ho', 'ho', 'ho', 'around', 'wonderful', 'time', 'year', 'neon', 'leader', 'retreat', 'time', 'know', 'time', 'year', 'extremely', 'hectic', 'tough', 'think', 'anything', 'past', 'holiday', 'life', 'go', 'past', 'week', 'december', '25', 'january', '1', 'like', 'think', 'minute', 'calender', 'handed', 'beginning', 'fall', 'semester', 'retreat', 'scheduled', 'weekend', 'january', '5', '6', 'youth', 'minister', 'conference', 'brad', 'dustin', 'connected', 'week', 'going', 'change', 'date', 'following', 'weekend', 'january', '12', '13', 'come', 'part', 'need', 'think', 'think', 'agree', 'important', 'u', 'get', 'together', 'time', 'recharge', 'battery', 'get', 'far', 'spring', 'semester', 'lot', 'trouble', 'difficult', 'u', 'get', 'away', 'without', 'kid', 'etc', 'brad', 'came', 'potential', 'alternative', 'get', 'together', 'weekend', 'let', 'know', 'prefer', 'first', 'option', 'would', 'retreat', 'similar', 'done', 'past', 'several', 'year', 'year', 'could', 'go', 'heartland', 'country', 'inn', 'www', 'com', 'outside', 'brenham', 'nice', 'place', '13', 'bedroom', '5', 'bedroom', 'house', 'side', 'side', 'country', 'real', 'relaxing', 'also', 'close', 'brenham', 'one', 'hour', '15', 'minute', 'golf', 'shop', 'antique', 'craft', 'store', 'brenham', 'eat', 'dinner', 'together', 'ranch', 'spend', 'time', 'meet', 'saturday', 'return', 'sunday', 'morning', 'like', 'done', 'past', 'second', 'option', 'would', 'stay', 'houston', 'dinner', 'together', 'nice', 'restaurant', 'dessert', 'time', 'visiting', 'recharging', 'one', 'home', 'saturday', 'evening', 'might', 'easier', 'trade', 'would', 'much', 'time', 'together', 'let', 'decide', 'email', 'back', 'would', 'preference', 'course', 'available', 'weekend', 'democratic', 'process', 'prevail', 'majority', 'vote', 'rule', 'let', 'hear', 'soon', 'possible', 'preferably', 'end', 'weekend', 'vote', 'go', 'way', 'complaining', 'allowed', 'like', 'tend', 'great', 'weekend', 'great', 'golf', 'great', 'fishing', 'great', 'shopping', 'whatever', 'make', 'happy', 'bobby']\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Apply stemming to cleaned word tokens\n",
        "df['stemmed_words'] = df['cleaned_nltk_word_tokens'].apply(lambda tokens: [stemmer.stem(t) for t in tokens])\n",
        "\n",
        "print(\"üîπ Sample before & after stemming:\\n\")\n",
        "for i in range(3):\n",
        "    print(f\"Original : {df['cleaned_nltk_word_tokens'][i]}\")\n",
        "    print(f\"Stemmed  : {df['stemmed_words'][i]}\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "\n",
        "# Apply lemmatization to cleaned word tokens\n",
        "df['lemmatized_words'] = df['cleaned_nltk_word_tokens'].apply(lambda tokens: [lemmatizer.lemmatize(t) for t in tokens])\n",
        "\n",
        "print(\"üîπ Sample before & after lemmatization:\\n\")\n",
        "for i in range(3):\n",
        "    print(f\"Original    : {df['cleaned_nltk_word_tokens'][i]}\")\n",
        "    print(f\"Lemmatized  : {df['lemmatized_words'][i]}\")\n",
        "    print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBZy3R7nvkoH",
        "outputId": "bbbfd09b-842e-4917-f960-3db4d105d43b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Top 10 Stemmed Tokens:\n",
            "[('ect', 13908), ('subject', 8064), ('hou', 7289), ('enron', 6555), ('2000', 4386), ('com', 3709), ('deal', 3655), ('pleas', 3243), ('ga', 3072), ('``', 3020)]\n",
            "\n",
            "üîπ Top 10 Lemmatized Tokens:\n",
            "[('ect', 13908), ('subject', 8062), ('hou', 7289), ('enron', 6555), ('2000', 4386), ('com', 3709), ('deal', 3635), ('please', 3198), ('gas', 3036), ('``', 3020)]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Flatten lists\n",
        "all_stemmed = [t for sublist in df['stemmed_words'] for t in sublist]\n",
        "all_lemmatized = [t for sublist in df['lemmatized_words'] for t in sublist]\n",
        "\n",
        "print(\"üîπ Top 10 Stemmed Tokens:\")\n",
        "print(Counter(all_stemmed).most_common(10))\n",
        "\n",
        "print(\"\\nüîπ Top 10 Lemmatized Tokens:\")\n",
        "print(Counter(all_lemmatized).most_common(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZb79mWC6I9_"
      },
      "source": [
        "Expt-3\n",
        "\n",
        "Exercise 1: Implementation of TF IDF Model for an input text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxYVA9-b52_5",
        "outputId": "7df29e8a-2076-4743-8132-3893f76bbb11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Words: ['The', 'runner', 'was', 'running', 'and', 'easily', 'outran', 'the', 'other', 'runners', '.']\n",
            "Stemmed Words: ['the', 'run', 'was', 'run', 'and', 'easy', 'out', 'the', 'oth', 'run', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Create Lancaster Stemmer object\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "# Example text\n",
        "text = \"The runner was running and easily outran the other runners.\"\n",
        "\n",
        "# Tokenize text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Apply Lancaster Stemmer\n",
        "stemmed_words = [lancaster.stem(word) for word in tokens]\n",
        "\n",
        "print(\"Original Words:\", tokens)\n",
        "print(\"Stemmed Words:\", stemmed_words)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG1HUbJi6mE6"
      },
      "source": [
        "Exercise 2: Implement N-Gram model for an input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnRWkK-t6BOP",
        "outputId": "c1ddbd21-1889-4492-d47e-4828dba6a4c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: ['‡§≤‡§°‡§º‡§ï‡§ø‡§Ø‡•ã‡§Ç', '‡§≤‡§°‡§º‡§ï‡§æ', '‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡§ø‡§Ø‡•ã‡§Ç', '‡§ï‡§ø‡§§‡§æ‡§¨‡•á‡§Ç', '‡§™‡§¢‡§º‡§æ‡§à']\n",
            "Stemmed: ['‡§≤‡§°‡§º‡§ï‡§ø‡§Ø', '‡§≤‡§°‡§º‡§ï', '‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡§ø‡§Ø', '‡§ï‡§ø‡§§‡§æ‡§¨', '‡§™‡§¢‡§º‡§æ‡§à']\n"
          ]
        }
      ],
      "source": [
        "# Simple Indic Stemmer for Hindi (Devanagari)\n",
        "\n",
        "def hindi_stemmer(word):\n",
        "    suffixes = [\"‡•ã‡§Ç\", \"‡•á‡§Ç\", \"‡•Ä‡§Ç\", \"‡§ø‡§è\", \"‡§ø‡§Ø‡•ã\", \"‡§ø‡§Ø‡§æ‡§Å\", \"‡§ø‡§Ø‡§æ\", \"‡§ø‡§Ø‡•á\", \"‡§ø‡§Ø‡•ã\", \"‡§æ‡§ì‡§Ç\", \"‡§æ‡§è‡§Å\", \"‡§æ‡§è‡§Ç\", \"‡§æ‡§§‡•á\", \"‡§æ‡§§‡§æ\", \"‡§æ‡§§‡•Ä\", \"‡§æ‡§®‡§æ\", \"‡§æ‡§®‡•á\", \"‡§æ‡§ï‡§∞\", \"‡§æ‡§ì\", \"‡•Ä\", \"‡§æ\"]\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[: -len(suffix)]\n",
        "    return word\n",
        "\n",
        "# Example Hindi text\n",
        "hindi_words = [\"‡§≤‡§°‡§º‡§ï‡§ø‡§Ø‡•ã‡§Ç\", \"‡§≤‡§°‡§º‡§ï‡§æ\", \"‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡§ø‡§Ø‡•ã‡§Ç\", \"‡§ï‡§ø‡§§‡§æ‡§¨‡•á‡§Ç\", \"‡§™‡§¢‡§º‡§æ‡§à\"]\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_hindi = [hindi_stemmer(word) for word in hindi_words]\n",
        "\n",
        "print(\"Original:\", hindi_words)\n",
        "print(\"Stemmed:\", stemmed_hindi)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg-kUve3yKH1"
      },
      "source": [
        "# Expt 4-  Bag of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D-G9U8OwYWm",
        "outputId": "9dc5ca1a-a661-4ff0-f949-a0c126fe10e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'original': 'running', 'lemma': 'running', 'morphy_root': 'running', 'pos_tags': ['s', 'v', 'n', 'a'], 'definitions': ['(American football) a play in which a player attempts to carry the ball through or past the opposing team', 'the act of running; traveling on foot at a fast pace', 'the state of being in operation', 'the act of administering or being in charge of something', 'the act of participating in an athletic competition involving running on a track', \"move fast by using one's feet, with one foot off the ground at any given time\", \"flee; take to one's heels; cut and run\", 'stretch out over a distance, space, time, or scope; run or extend between two points or beyond a certain point', 'direct or control; projects, businesses, etc.', 'have a particular form', 'move along, of liquids', 'perform as expected when applied', 'change or be different within limits', 'run, stand, or compete for an office or a position', 'cause to emit recorded audio or video', 'move about freely and without restraint, or act as if running around in an uncontrolled way', 'have a tendency or disposition to do or be something; be inclined', 'be operating, running or functioning', 'change from one state to another', 'cause to perform', 'be affected by; be subjected to', 'continue to exist', 'occur persistently', 'carry out a process or program, as on a computer or a machine', 'include as the content; broadcast or publicize', 'carry out', 'pass over, across, or through', 'cause something to pass or lead somewhere', 'make without a miss', 'deal in illegally, such as arms or liquor', 'cause an animal to move fast', 'be diffused', 'sail before the wind', 'cover by running; run a certain distance', 'extend or continue for a certain period of time', 'set animals loose to graze', 'keep company', 'run with the ball; in such sports as football', 'travel rapidly, by any (unspecified) means', 'travel a route regularly', 'pursue for food or sport (as of wild animals)', 'compete in a race', 'progress by being changed', 'reduce or cause to be reduced from a solid to a liquid state, usually by heating', 'come unraveled or undone as if by snagging', 'become undone', '(of fluids) moving or issuing in a stream', 'continually repeated over a period of time', 'of advancing the ball by running', 'executed or initiated by running', 'measured lengthwise', '(of e.g. a machine) performing or capable of performing']}\n",
            "--------------------------------------------------\n",
            "{'original': 'better', 'lemma': 'better', 'morphy_root': 'better', 'pos_tags': ['s', 'r', 'a', 'v', 'n'], 'definitions': ['something superior in quality or condition or effect', 'someone who bets', 'a superior person having claim to precedence', 'the superior one of two alternatives', 'surpass in excellence', 'to make better', 'get better', \"(comparative of `good') superior to another (of the same class or set or kind) in excellence or quality or desirability or suitability; more highly skilled than another\", \"(comparative of `good') changed for the better in health or fitness\", \"(comparative and superlative of `well') wiser or more advantageous and hence advisable\", 'more than half', 'having desirable or positive qualities especially those suitable for a thing specified', 'having the normally expected amount', 'morally admirable', 'deserving of esteem and respect', 'promoting or enhancing well-being', 'agreeable or pleasing', 'of moral excellence', 'having or showing knowledge and skill and aptitude', 'thorough', 'with or in a close or intimate relationship', 'financially sound', 'most suitable or right for a particular purpose', 'resulting favorably', 'exerting force or influence', 'capable of pleasing', 'appealing to the mind', 'in excellent physical condition', 'tending to promote physical well-being; beneficial to health', 'not forged', 'not left to spoil', 'generally admired', 'in good health especially after having suffered illness or injury', 'resulting favorably', 'wise or advantageous and hence advisable', \"comparative of `well'; in a better or more excellent manner or more advantageously or attractively or to a greater degree etc.\", 'from a position of superiority or authority', \"(often used as a combining form) in a good or proper or satisfactory manner or to a high standard (`good' is a nonstandard dialectal variant for `well')\", 'thoroughly or completely; fully; often used as a combining form; ; ; ; ; ,', 'indicating high probability; in all likelihood', '(used for emphasis or as an intensifier) entirely or fully', 'to a suitable or appropriate extent or degree', 'favorably; with approval', 'to a great extent or degree', 'with great or especially intimate knowledge', 'with prudence or propriety', 'with skill or in a pleasing manner', 'in a manner affording benefit or advantage', 'in financial comfort', 'without unusual distress or resentment; with good humor']}\n",
            "--------------------------------------------------\n",
            "{'original': 'studies', 'lemma': 'study', 'morphy_root': 'study', 'pos_tags': ['v', 'n'], 'definitions': ['a detailed critical inspection', 'applying the mind to learning and understanding a subject (especially by reading)', 'a written document describing the findings of some individual or group', 'a state of deep mental absorption', 'a room used for reading and writing and studying', 'a branch of knowledge', 'preliminary drawing for later elaboration', 'attentive consideration and meditation', 'someone who memorizes quickly and easily (as the lines for a part in a play)', \"a composition intended to develop one aspect of the performer's technique\", 'consider in detail and subject to an analysis in order to discover essential features or meaning', 'be a student; follow a course of study; be enrolled at an institute of learning', 'give careful consideration to', 'be a student of a certain subject', 'learn by reading books', 'think intently and at length, as for spiritual purposes']}\n",
            "--------------------------------------------------\n",
            "{'original': 'flying', 'lemma': 'flying', 'morphy_root': 'flying', 'pos_tags': ['s', 'v', 'n'], 'definitions': ['an instance of traveling by air', 'travel through the air; be airborne', 'move quickly or suddenly', 'operate an airplane', 'transport by aeroplane', 'cause to fly or float', 'be dispersed or disseminated', 'change quickly from one emotional state to another', 'pass away rapidly', 'travel in an airplane', 'display in the air or cause to float', 'run away quickly', 'travel over (an area of land or sea) in an aircraft', 'hit a fly', 'decrease rapidly and disappear', 'moving swiftly', 'hurried and brief']}\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def morphological_analysis(word):\n",
        "    # Get lemma/root form\n",
        "    lemma = lemmatizer.lemmatize(word)\n",
        "\n",
        "    # Get morphological root using morphy\n",
        "    morphy_root = wordnet.morphy(word)\n",
        "\n",
        "    # Get all synsets for the word\n",
        "    synsets = wordnet.synsets(word)\n",
        "\n",
        "    return {\n",
        "        \"original\": word,\n",
        "        \"lemma\": lemma,\n",
        "        \"morphy_root\": morphy_root,\n",
        "        \"pos_tags\": list(set([s.pos() for s in synsets])) if synsets else [],\n",
        "        \"definitions\": [s.definition() for s in synsets]\n",
        "    }\n",
        "    # Example\n",
        "sample_words = [\"running\", \"better\", \"studies\", \"flying\"]\n",
        "for w in sample_words:\n",
        "    print(morphological_analysis(w))\n",
        "    print(\"-\"*50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP1CTSFNwZF5",
        "outputId": "74ba740b-02b0-4507-c8be-798c10e63849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word: running\n",
            "Generated Forms: {'take_to_the_woods', 'move', 'melt', 'play', 'black_market', 'hunt_down', 'melt_down', 'track', 'running_game', 'tend', 'work', 'break_away', 'pass', 'race', 'bunk', 'escape', 'lead', 'campaign', 'execute', 'carry', 'linear', 'flow', 'feed', 'ply', 'operate', 'unravel', 'head_for_the_hills', 'die_hard', 'hunt', 'course', 'idle', 'range', 'ladder', 'passing', 'operative', 'run', 'prevail', 'scarper', 'run_for', 'extend', 'endure', 'standing', 'track_down', 'function', 'go', 'consort', 'run_away', 'be_given', 'incline', 'bleed', 'turn_tail', 'running_play', 'persist', 'running', 'lam', 'functional', 'draw', 'scat', 'lean', 'working', 'hightail_it', 'guide', 'fly_the_coop', 'malfunction'}\n",
            "--------------------------------------------------\n",
            "Word: better\n",
            "Generated Forms: {'break', 'amend', 'secure', 'effective', 'salutary', 'undecomposed', 'unspoiled', 'advantageously', 'badly', 'respectable', 'best', 'evil', 'worsen', 'serious', 'bad', 'beneficial', 'dependable', 'considerably', 'intimately', 'comfortably', 'in_force', 'improve', 'expert', 'dear', 'full', 'honorable', 'well', 'substantially', 'better', 'near', 'unspoilt', 'bettor', 'estimable', 'safe', 'easily', 'adept', 'skillful', 'punter', 'wagerer', 'in_effect', 'disadvantageously', 'ameliorate', 'worse', 'just', 'right', 'practiced', 'upright', 'ill', 'proficient', 'honest', 'skilful', 'good', 'ripe', 'meliorate', 'sound'}\n",
            "--------------------------------------------------\n",
            "Word: studies\n",
            "Generated Forms: {'canvas', 'examine', 'consider', 'analyse', 'discipline', 'hit_the_books', 'work', 'read', 'study', 'field', 'bailiwick', 'analyze', 'canvass', 'report', 'take', 'cogitation', 'learn', 'contemplate', 'subject', 'written_report', 'subject_field', 'survey', 'sketch', 'meditate', 'subject_area', 'field_of_study'}\n",
            "--------------------------------------------------\n",
            "Word: flying\n",
            "Generated Forms: {'flight', 'vaporize', 'pilot', 'fell', 'fast', 'fast-flying', 'aviate', 'wing', 'vanish', 'flee', 'take_flight', 'quick', 'flying', 'fly'}\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def generate_word_forms(word):\n",
        "    forms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            forms.add(lemma.name())  # Add the lemma form\n",
        "            if lemma.antonyms():     # Add antonyms if present\n",
        "                forms.add(lemma.antonyms()[0].name())\n",
        "    return forms\n",
        "# Example\n",
        "for w in sample_words:\n",
        "    print(f\"Word: {w}\")\n",
        "    print(\"Generated Forms:\", generate_word_forms(w))\n",
        "    print(\"-\"*50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0M1s20Rwdh5",
        "outputId": "a9ea8b14-c298-4ddc-9824-e8a18d95fe7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Morphological Analysis for sample dataset words:\n",
            "\n",
            "{'original': 'incubi', 'lemma': 'incubus', 'morphy_root': 'incubus', 'pos_tags': ['n'], 'definitions': ['a male demon believed to lie on sleeping persons and to have sexual intercourse with sleeping women', 'a situation resembling a terrifying dream', 'someone who depresses or worries others']}\n",
            "Generated Forms: {'nightmare', 'incubus'}\n",
            "============================================================\n",
            "{'original': '1871', 'lemma': '1871', 'morphy_root': None, 'pos_tags': [], 'definitions': []}\n",
            "Generated Forms: set()\n",
            "============================================================\n",
            "{'original': 'tianhe', 'lemma': 'tianhe', 'morphy_root': None, 'pos_tags': [], 'definitions': []}\n",
            "Generated Forms: set()\n",
            "============================================================\n",
            "{'original': 'ahe', 'lemma': 'ahe', 'morphy_root': None, 'pos_tags': [], 'definitions': []}\n",
            "Generated Forms: set()\n",
            "============================================================\n",
            "{'original': 'gratuity', 'lemma': 'gratuity', 'morphy_root': 'gratuity', 'pos_tags': ['n'], 'definitions': ['a relatively small amount of money given for services rendered (as by a waiter)', 'an award (as for meritorious service) given without claim or obligation']}\n",
            "Generated Forms: {'gratuity', 'tip', 'baksheesh', 'bakshis', 'backsheesh', 'bakshish', 'pourboire'}\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Take first 5 unique cleaned words from the dataset\n",
        "unique_words = list(set([w for sublist in df['cleaned_nltk_word_tokens'] for w in sublist]))[:5]\n",
        "\n",
        "print(\"Morphological Analysis for sample dataset words:\\n\")\n",
        "for w in unique_words:\n",
        "    print(morphological_analysis(w))\n",
        "    print(\"Generated Forms:\", generate_word_forms(w))\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Icx8j8-fyQe8"
      },
      "source": [
        "# Expt 4 - TFIDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pskovF3XyuuD",
        "outputId": "14968848-d3e5-419c-bdec-09f230eda3b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        and       are       cat      cats       dog      dogs       log  \\\n",
            "0  0.000000  0.000000  0.427554  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.000000  0.000000  0.427554  0.000000  0.427554   \n",
            "2  0.447214  0.447214  0.000000  0.447214  0.000000  0.447214  0.000000   \n",
            "\n",
            "        mat        on      pets       sat       the  \n",
            "0  0.427554  0.325166  0.000000  0.325166  0.650331  \n",
            "1  0.000000  0.325166  0.000000  0.325166  0.650331  \n",
            "2  0.000000  0.000000  0.447214  0.000000  0.000000  \n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Example corpus\n",
        "corpus = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"The dog sat on the log\",\n",
        "    \"Cats and dogs are pets\"\n",
        "]\n",
        "\n",
        "# Create the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert to DataFrame for better readability\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHbRFVIz5I_s"
      },
      "source": [
        "Expt 4 - N-Gram\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txxj33fi4fGz",
        "outputId": "dc6d1f11-a474-4ffa-cc4c-1c094e043f96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Top 10 Most Common Bigrams:\n",
            "('-', '-'): 65612\n",
            "('subject', ':'): 7854\n",
            "('/', 'ect'): 7313\n",
            "('/', 'hou'): 7278\n",
            "('hou', '/'): 7278\n",
            "('@', 'ect'): 6547\n",
            "('ect', '@'): 6420\n",
            "('.', '.'): 4350\n",
            "('ect', ','): 4278\n",
            "('>', '>'): 3810\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "üîπ Top 10 Most Common Trigrams:\n",
            "('-', '-', '-'): 61156\n",
            "('/', 'hou', '/'): 7278\n",
            "('hou', '/', 'ect'): 7226\n",
            "('/', 'ect', '@'): 6420\n",
            "('ect', '@', 'ect'): 6338\n",
            "('@', 'ect', ','): 4241\n",
            "('.', '.', '.'): 3180\n",
            "('>', '>', '>'): 2817\n",
            "('?', '?', '?'): 2810\n",
            "('=', '=', '='): 1726\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure 'punkt' is downloaded for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Reload the original dataframe as it was overwritten in a previous cell\n",
        "df = pd.read_csv(\"/content/spam_ham_dataset.csv\", encoding='latin-1')\n",
        "\n",
        "# Step 1: Tokenize the text column into words (if not done already)\n",
        "df['cleaned_nltk_word_tokens'] = df['text'].apply(lambda x: word_tokenize(str(x).lower()))\n",
        "\n",
        "# Step 2: Function to get N-Grams from tokenized text\n",
        "def get_ngrams(text_tokens, n):\n",
        "    return list(ngrams(text_tokens, n))\n",
        "\n",
        "# Step 3: Create bigrams and trigrams\n",
        "all_bigrams = []\n",
        "all_trigrams = []\n",
        "\n",
        "# Step 4: Loop through each tokenized text to generate n-grams\n",
        "for tokens in df['cleaned_nltk_word_tokens']:\n",
        "    all_bigrams.extend(get_ngrams(tokens, 2))  # Generate bigrams (n=2)\n",
        "    all_trigrams.extend(get_ngrams(tokens, 3))  # Generate trigrams (n=3)\n",
        "\n",
        "# Step 5: Frequency distribution\n",
        "bigram_freq = Counter(all_bigrams)\n",
        "trigram_freq = Counter(all_trigrams)\n",
        "\n",
        "# Step 6: Show top 10 bigrams and trigrams\n",
        "print(\"üîπ Top 10 Most Common Bigrams:\")\n",
        "for bigram, count in bigram_freq.most_common(10):\n",
        "    print(f\"{bigram}: {count}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "print(\"üîπ Top 10 Most Common Trigrams:\")\n",
        "for trigram, count in trigram_freq.most_common(10):\n",
        "    print(f\"{trigram}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH6EeW3r7ouI"
      },
      "source": [
        "EXP 5 Pos Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l0dg-HD7xqQ",
        "outputId": "13434f6b-3856-4cde-c893-51aea3383e1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            cleaned_nltk_word_tokens  \\\n",
            "0  [subject, :, enron, methanol, ;, meter, #, :, ...   \n",
            "1  [subject, :, hpl, nom, for, january, 9, ,, 200...   \n",
            "2  [subject, :, neon, retreat, ho, ho, ho, ,, we,...   \n",
            "3  [subject, :, photoshop, ,, windows, ,, office,...   \n",
            "4  [subject, :, re, :, indian, springs, this, dea...   \n",
            "\n",
            "                                            pos_tags  \n",
            "0  [(subject, NN), (:, :), (enron, NN), (methanol...  \n",
            "1  [(subject, NN), (:, :), (hpl, NN), (nom, NN), ...  \n",
            "2  [(subject, NN), (:, :), (neon, NN), (retreat, ...  \n",
            "3  [(subject, NN), (:, :), (photoshop, NN), (,, ,...  \n",
            "4  [(subject, NN), (:, :), (re, NN), (:, :), (ind...  \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab') # Added to download the missing resource\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Added to download the missing resource\n",
        "\n",
        "\n",
        "# Reload the original dataframe as it was overwritten or not loaded in the current runtime\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/spam_ham_dataset.csv\", encoding='latin-1')\n",
        "from nltk.tokenize import word_tokenize\n",
        "df['cleaned_nltk_word_tokens'] = df['text'].apply(lambda x: word_tokenize(str(x).lower()))\n",
        "\n",
        "# Assuming 'cleaned_nltk_word_tokens' is a column in your DataFrame containing lists of word tokens\n",
        "df['pos_tags'] = df['cleaned_nltk_word_tokens'].apply(lambda tokens: nltk.tag.pos_tag(tokens))\n",
        "\n",
        "# Display the updated DataFrame with POS tags\n",
        "print(df[['cleaned_nltk_word_tokens', 'pos_tags']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubYN0Q3QA7H0"
      },
      "source": [
        "Exp 6 Chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHzo4qvrA-7y",
        "outputId": "556fe13d-afc1-4777-c8d9-c17ccc689a8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Chunk Trees for the first 3 entries:\n",
            "\n",
            "--- Entry 1 ---\n",
            "Text-based Tree:\n",
            "(S\n",
            "  (NP subject/NN)\n",
            "  :/:\n",
            "  (NP enron/NN methanol/NN)\n",
            "  ;/:\n",
            "  meter/CC\n",
            "  #/#\n",
            "  :/:\n",
            "  988291/CD\n",
            "  (NP this/DT)\n",
            "  (VP is/VBZ (NP a/DT follow/JJ))\n",
            "  up/RB\n",
            "  to/TO\n",
            "  (NP the/DT note/NN i/NN)\n",
            "  (VP gave/VBD)\n",
            "  you/PRP\n",
            "  (PP on/IN (NP monday/NN))\n",
            "  ,/,\n",
            "  4/CD\n",
            "  (NP //NN)\n",
            "  3/CD\n",
            "  (NP //NN)\n",
            "  00/CD\n",
            "  {/(\n",
            "  (NP preliminary/JJ flow/NN data/NNS)\n",
            "  (VP provided/VBN)\n",
            "  (PP by/IN (NP daren/NN))\n",
            "  }/)\n",
            "  ./.\n",
            "  (VP please/VB (NP override/JJ pop/NN))\n",
            "  '/POS\n",
            "  (NP s/JJ daily/JJ volume/NN)\n",
            "  {/(\n",
            "  presently/RB\n",
            "  zero/CD\n",
            "  }/)\n",
            "  to/TO\n",
            "  (VP reflect/VB (NP daily/JJ activity/NN))\n",
            "  you/PRP\n",
            "  can/MD\n",
            "  (VP obtain/VB)\n",
            "  (PP from/IN (NP gas/NN control/NN))\n",
            "  ./.\n",
            "  (NP this/DT change/NN)\n",
            "  (VP is/VBZ)\n",
            "  (VP needed/VBN (NP asap/NN))\n",
            "  (PP for/IN (NP economics/NN purposes/NNS))\n",
            "  ./.)\n",
            "--------------------\n",
            "Could not generate graphical tree for Entry 1: no display name and no $DISPLAY environment variable\n",
            "Graphical visualization in Colab can be challenging.\n",
            "\n",
            "--- Entry 2 ---\n",
            "Text-based Tree:\n",
            "(S\n",
            "  (NP subject/NN)\n",
            "  :/:\n",
            "  (NP hpl/NN nom/NN)\n",
            "  (PP for/IN (NP january/JJ))\n",
            "  9/CD\n",
            "  ,/,\n",
            "  2001/CD\n",
            "  (/(\n",
            "  (VP see/VB)\n",
            "  (VP attached/VBN (NP file/NN))\n",
            "  :/:\n",
            "  (NP hplnol/NN)\n",
            "  09/CD\n",
            "  ./.\n",
            "  (NP xls/NN)\n",
            "  )/)\n",
            "  -/:\n",
            "  (NP hplnol/NN)\n",
            "  09/CD\n",
            "  ./.\n",
            "  (NP xls/NN))\n",
            "--------------------\n",
            "Could not generate graphical tree for Entry 2: no display name and no $DISPLAY environment variable\n",
            "Graphical visualization in Colab can be challenging.\n",
            "\n",
            "--- Entry 3 ---\n",
            "Text-based Tree:\n",
            "(S\n",
            "  (NP subject/NN)\n",
            "  :/:\n",
            "  (NP neon/NN retreat/NN ho/NN ho/NN ho/NN)\n",
            "  ,/,\n",
            "  we/PRP\n",
            "  '/''\n",
            "  (VP re/VBP)\n",
            "  around/IN\n",
            "  to/TO\n",
            "  (NP that/DT)\n",
            "  most/RBS\n",
            "  (NP wonderful/JJ time/NN)\n",
            "  (PP of/IN (NP the/DT year/NN))\n",
            "  -/:\n",
            "  -/:\n",
            "  -/:\n",
            "  (NP neon/NN leaders/NNS)\n",
            "  (VP retreat/VBP (NP time/NN))\n",
            "  !/.\n",
            "  (NP i/NN)\n",
            "  (VP know/VBP)\n",
            "  (PP that/IN (NP this/DT time/NN))\n",
            "  (PP of/IN (NP year/NN))\n",
            "  (VP is/VBZ)\n",
            "  extremely/RB\n",
            "  (NP hectic/JJ)\n",
            "  ,/,\n",
            "  and/CC\n",
            "  that/IN\n",
            "  it/PRP\n",
            "  '/''\n",
            "  (NP s/JJ tough/JJ)\n",
            "  to/TO\n",
            "  (VP think/VB)\n",
            "  (PP about/IN (NP anything/NN))\n",
            "  (PP past/IN (NP the/DT holidays/NNS))\n",
            "  ,/,\n",
            "  but/CC\n",
            "  (NP life/NN)\n",
            "  (VP does/VBZ)\n",
            "  (VP go/VB)\n",
            "  on/IN\n",
            "  (PP past/IN (NP the/DT week/NN))\n",
            "  (PP of/IN (NP december/NN))\n",
            "  25/CD\n",
            "  (PP through/IN (NP january/JJ))\n",
            "  1/CD\n",
            "  ,/,\n",
            "  and/CC\n",
            "  that/IN\n",
            "  '/''\n",
            "  (VP s/VB)\n",
            "  what/WP\n",
            "  (NP i/NN)\n",
            "  '/''\n",
            "  (NP d/NNS)\n",
            "  like/IN\n",
            "  you/PRP\n",
            "  to/TO\n",
            "  (VP think/VB)\n",
            "  about/IN\n",
            "  (PP for/IN (NP a/DT minute/NN))\n",
            "  ./.\n",
            "  (PP on/IN (NP the/DT calender/NN))\n",
            "  (PP that/IN (NP i/NN))\n",
            "  (VP handed/VBD)\n",
            "  out/RP\n",
            "  (PP at/IN (NP the/DT beginning/NN))\n",
            "  (PP of/IN (NP the/DT fall/NN semester/NN))\n",
            "  ,/,\n",
            "  (NP the/DT retreat/NN)\n",
            "  (VP was/VBD)\n",
            "  (VP scheduled/VBN)\n",
            "  (PP for/IN (NP the/DT weekend/NN))\n",
            "  (PP of/IN (NP january/JJ))\n",
            "  5/CD\n",
            "  -/:\n",
            "  6/CD\n",
            "  ./.\n",
            "  but/CC\n",
            "  because/IN\n",
            "  (PP of/IN (NP a/DT youth/NN ministers/NNS conference/NN))\n",
            "  (PP that/IN (NP brad/NN))\n",
            "  and/CC\n",
            "  (NP dustin/NN)\n",
            "  (VP are/VBP)\n",
            "  (VP connected/VBN)\n",
            "  (PP with/IN (NP that/DT week/NN))\n",
            "  ,/,\n",
            "  we/PRP\n",
            "  '/''\n",
            "  (NP re/NN)\n",
            "  (VP going/VBG)\n",
            "  to/TO\n",
            "  (VP change/VB (NP the/DT date/NN))\n",
            "  to/TO\n",
            "  (NP the/DT following/JJ weekend/NN)\n",
            "  ,/,\n",
            "  (NP january/JJ)\n",
            "  12/CD\n",
            "  -/:\n",
            "  13/CD\n",
            "  ./.\n",
            "  now/RB\n",
            "  (VP comes/VBZ (NP the/DT part/NN))\n",
            "  you/PRP\n",
            "  (VP need/VBP)\n",
            "  to/TO\n",
            "  (VP think/VB)\n",
            "  about/IN\n",
            "  ./.\n",
            "  (NP i/JJ)\n",
            "  (VP think/VBP)\n",
            "  we/PRP\n",
            "  (NP all/DT)\n",
            "  (VP agree/VBP)\n",
            "  that/IN\n",
            "  it/PRP\n",
            "  '/''\n",
            "  (NP s/JJ important/JJ)\n",
            "  for/IN\n",
            "  us/PRP\n",
            "  to/TO\n",
            "  (VP get/VB)\n",
            "  together/RB\n",
            "  and/CC\n",
            "  (VP have/VB (NP some/DT time/NN))\n",
            "  to/TO\n",
            "  (VP recharge/VB)\n",
            "  our/PRP$\n",
            "  (NP batteries/NNS)\n",
            "  before/IN\n",
            "  we/PRP\n",
            "  (VP get/VBP)\n",
            "  to/TO\n",
            "  far/RB\n",
            "  (PP into/IN (NP the/DT spring/NN semester/NN))\n",
            "  ,/,\n",
            "  but/CC\n",
            "  it/PRP\n",
            "  can/MD\n",
            "  (VP be/VB (NP a/DT lot/NN))\n",
            "  (PP of/IN (NP trouble/NN))\n",
            "  and/CC\n",
            "  (NP difficult/JJ)\n",
            "  for/IN\n",
            "  us/PRP\n",
            "  to/TO\n",
            "  (VP get/VB)\n",
            "  away/RB\n",
            "  (PP without/IN (NP kids/NNS))\n",
            "  ,/,\n",
            "  etc/FW\n",
            "  ./.\n",
            "  so/RB\n",
            "  ,/,\n",
            "  (NP brad/NN)\n",
            "  (VP came/VBD)\n",
            "  up/RP\n",
            "  (PP with/IN (NP a/DT potential/JJ alternative/NN))\n",
            "  for/IN\n",
            "  how/WRB\n",
            "  we/PRP\n",
            "  can/MD\n",
            "  (VP get/VB)\n",
            "  together/RB\n",
            "  (PP on/IN (NP that/DT weekend/NN))\n",
            "  ,/,\n",
            "  and/CC\n",
            "  then/RB\n",
            "  you/PRP\n",
            "  can/MD\n",
            "  (VP let/VB)\n",
            "  me/PRP\n",
            "  (VP know/VB)\n",
            "  which/WDT\n",
            "  you/PRP\n",
            "  (VP prefer/VBP)\n",
            "  ./.\n",
            "  (NP the/DT first/JJ option/NN)\n",
            "  would/MD\n",
            "  (VP be/VB)\n",
            "  to/TO\n",
            "  (VP have/VB (NP a/DT retreat/NN similar/JJ))\n",
            "  to/TO\n",
            "  what/WP\n",
            "  we/PRP\n",
            "  '/''\n",
            "  (NP ve/NN)\n",
            "  (VP done/VBN (NP the/DT past/JJ several/JJ years/NNS))\n",
            "  ./.\n",
            "  (NP this/DT year/NN)\n",
            "  we/PRP\n",
            "  could/MD\n",
            "  (VP go/VB)\n",
            "  to/TO\n",
            "  (NP the/DT heartland/NN country/NN inn/NN)\n",
            "  (/(\n",
            "  (NP www/NN)\n",
            "  ./.\n",
            "  ./.\n",
            "  (NP com/NN)\n",
            "  )/)\n",
            "  outside/IN\n",
            "  (PP of/IN (NP brenham/NN))\n",
            "  ./.\n",
            "  it/PRP\n",
            "  '/''\n",
            "  (VP s/VBZ (NP a/DT nice/JJ place/NN))\n",
            "  ,/,\n",
            "  where/WRB\n",
            "  we/PRP\n",
            "  '/''\n",
            "  (NP d/NNS)\n",
            "  (VP have/VBP (NP a/DT))\n",
            "  13/CD\n",
            "  -/:\n",
            "  (NP bedroom/NN)\n",
            "  and/CC\n",
            "  (NP a/DT)\n",
            "  5/CD\n",
            "  -/:\n",
            "  (NP bedroom/NN house/NN side/NN)\n",
            "  (PP by/IN (NP side/NN))\n",
            "  ./.\n",
            "  it/PRP\n",
            "  '/''\n",
            "  (NP s/NN)\n",
            "  (PP in/IN (NP the/DT country/NN))\n",
            "  ,/,\n",
            "  (NP real/JJ relaxing/NN)\n",
            "  ,/,\n",
            "  but/CC\n",
            "  also/RB\n",
            "  close/RB\n",
            "  to/TO\n",
            "  (VP brenham/VB)\n",
            "  and/CC\n",
            "  only/RB\n",
            "  about/IN\n",
            "  one/CD\n",
            "  (NP hour/NN)\n",
            "  and/CC\n",
            "  15/CD\n",
            "  (NP minutes/NNS)\n",
            "  from/IN\n",
            "  here/RB\n",
            "  ./.\n",
            "  we/PRP\n",
            "  can/MD\n",
            "  (VP golf/VB)\n",
            "  ,/,\n",
            "  (NP shop/NN)\n",
            "  (PP in/IN (NP the/DT antique/NN))\n",
            "  and/CC\n",
            "  (NP craft/NN stores/NNS)\n",
            "  (PP in/IN (NP brenham/NN))\n",
            "  ,/,\n",
            "  (NP eat/NN dinner/NN)\n",
            "  together/RB\n",
            "  (PP at/IN (NP the/DT ranch/NN))\n",
            "  ,/,\n",
            "  and/CC\n",
            "  (NP spend/JJ time/NN)\n",
            "  (PP with/IN (NP each/DT other/JJ))\n",
            "  ./.\n",
            "  we/PRP\n",
            "  '/''\n",
            "  (NP d/JJ meet/NN)\n",
            "  (PP on/IN (NP saturday/NN))\n",
            "  ,/,\n",
            "  and/CC\n",
            "  then/RB\n",
            "  (VP return/VB)\n",
            "  (PP on/IN (NP sunday/JJ morning/NN))\n",
            "  ,/,\n",
            "  just/RB\n",
            "  like/IN\n",
            "  what/WP\n",
            "  we/PRP\n",
            "  '/''\n",
            "  (NP ve/JJ)\n",
            "  (VP done/VBN)\n",
            "  (PP in/IN (NP the/DT past/NN))\n",
            "  ./.\n",
            "  (NP the/DT second/JJ option/NN)\n",
            "  would/MD\n",
            "  (VP be/VB)\n",
            "  to/TO\n",
            "  (VP stay/VB)\n",
            "  here/RB\n",
            "  (PP in/IN (NP houston/NN))\n",
            "  ,/,\n",
            "  (VP have/VBP)\n",
            "  (VP dinner/VBN)\n",
            "  together/RB\n",
            "  (PP at/IN (NP a/DT nice/JJ restaurant/NN))\n",
            "  ,/,\n",
            "  and/CC\n",
            "  then/RB\n",
            "  (VP have/VB (NP dessert/NN))\n",
            "  and/CC\n",
            "  (NP a/DT time/NN)\n",
            "  for/IN\n",
            "  (VP visiting/VBG)\n",
            "  and/CC\n",
            "  (VP recharging/VBG)\n",
            "  at/IN\n",
            "  one/CD\n",
            "  of/IN\n",
            "  our/PRP$\n",
            "  (NP homes/NNS)\n",
            "  (PP on/IN (NP that/DT saturday/JJ evening/NN))\n",
            "  ./.\n",
            "  (NP this/DT)\n",
            "  might/MD\n",
            "  (VP be/VB)\n",
            "  easier/JJR\n",
            "  ,/,\n",
            "  but/CC\n",
            "  (NP the/DT trade/NN)\n",
            "  off/RP\n",
            "  would/MD\n",
            "  (VP be/VB)\n",
            "  that/IN\n",
            "  we/PRP\n",
            "  (VP wouldn/VBP)\n",
            "  '/''\n",
            "  (NP t/NNS)\n",
            "  (VP have/VBP)\n",
            "  as/RB\n",
            "  (NP much/JJ time/NN)\n",
            "  together/RB\n",
            "  ./.\n",
            "  (NP i/NN)\n",
            "  '/''\n",
            "  (NP ll/JJ let/NN)\n",
            "  you/PRP\n",
            "  (VP decide/VBP)\n",
            "  ./.\n",
            "  (VP email/VB)\n",
            "  me/PRP\n",
            "  back/RP\n",
            "  with/IN\n",
            "  what/WP\n",
            "  would/MD\n",
            "  (VP be/VB)\n",
            "  your/PRP$\n",
            "  (NP preference/NN)\n",
            "  ,/,\n",
            "  and/CC\n",
            "  (PP of/IN (NP course/NN))\n",
            "  if/IN\n",
            "  you/PRP\n",
            "  (VP '/VBP (NP re/JJ available/JJ))\n",
            "  (PP on/IN (NP that/DT weekend/NN))\n",
            "  ./.\n",
            "  (NP the/DT democratic/JJ process/NN)\n",
            "  will/MD\n",
            "  (VP prevail/VB)\n",
            "  -/:\n",
            "  -/:\n",
            "  (NP majority/NN vote/NN)\n",
            "  will/MD\n",
            "  (VP rule/VB)\n",
            "  !/.\n",
            "  (VP let/VB)\n",
            "  me/PRP\n",
            "  (VP hear/VB)\n",
            "  from/IN\n",
            "  you/PRP\n",
            "  as/RB\n",
            "  soon/RB\n",
            "  (PP as/IN (NP possible/JJ))\n",
            "  ,/,\n",
            "  preferably/RB\n",
            "  (PP by/IN (NP the/DT end/NN))\n",
            "  (PP of/IN (NP the/DT weekend/NN))\n",
            "  ./.\n",
            "  and/CC\n",
            "  (PP if/IN (NP the/DT vote/NN doesn/NN))\n",
            "  '/''\n",
            "  (NP t/NNS)\n",
            "  (VP go/VB)\n",
            "  your/PRP$\n",
            "  (NP way/NN)\n",
            "  ,/,\n",
            "  (NP no/DT complaining/NN)\n",
            "  (VP allowed/VBD)\n",
            "  (/(\n",
            "  (PP like/IN (NP i/NN))\n",
            "  (VP tend/VBP)\n",
            "  to/TO\n",
            "  (VP do/VB)\n",
            "  !/.\n",
            "  )/)\n",
            "  (VP have/VBP (NP a/DT great/JJ weekend/NN))\n",
            "  ,/,\n",
            "  (NP great/JJ golf/NN)\n",
            "  ,/,\n",
            "  (NP great/JJ fishing/NN)\n",
            "  ,/,\n",
            "  (NP great/JJ shopping/NN)\n",
            "  ,/,\n",
            "  or/CC\n",
            "  whatever/WDT\n",
            "  (VP makes/VBZ)\n",
            "  you/PRP\n",
            "  (NP happy/JJ)\n",
            "  !/.\n",
            "  (NP bobby/NN))\n",
            "--------------------\n",
            "Could not generate graphical tree for Entry 3: no display name and no $DISPLAY environment variable\n",
            "Graphical visualization in Colab can be challenging.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.chunk import RegexpParser\n",
        "from nltk.tree import Tree\n",
        "import os # Import the os module\n",
        "\n",
        "# Assuming 'pos_tags' is a column in your DataFrame containing lists of (word, pos_tag) tuples\n",
        "# If not, you'll need to perform POS tagging first (as done in the previous cell)\n",
        "\n",
        "# Define a simple chunk grammar\n",
        "grammar = r\"\"\"\n",
        "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
        "  VP: {<VB.*><NP|PP|CLAUSE>*} # Chunk verbs and their complements\n",
        "  PP: {<IN><NP>}              # Chunk prepositions followed by noun phrases\n",
        "  ENTITY: {<NNP|NNPS>+}      # Chunk sequences of one or more proper nouns (singular or plural)\n",
        "\"\"\"\n",
        "\n",
        "# Create a RegexpParser with the defined grammar\n",
        "chunk_parser = RegexpParser(grammar)\n",
        "\n",
        "# Process and display chunk trees for the first few entries in the DataFrame\n",
        "num_samples = 3 # Display chunk trees for the first few entries to avoid excessive output\n",
        "\n",
        "if not df.empty and 'pos_tags' in df.columns:\n",
        "    print(f\"üîπ Chunk Trees for the first {num_samples} entries:\")\n",
        "    for i in range(min(num_samples, len(df))):\n",
        "        pos_tags = df['pos_tags'].iloc[i]\n",
        "        if pos_tags: # Check if the list of pos_tags is not empty\n",
        "            chunk_tree = chunk_parser.parse(pos_tags)\n",
        "\n",
        "            print(f\"\\n--- Entry {i+1} ---\")\n",
        "            print(\"Text-based Tree:\")\n",
        "            print(chunk_tree)\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "            # Attempt to save the tree to a file for graphical visualization\n",
        "            try:\n",
        "                output_filename = f\"chunk_tree_entry_{i+1}.ps\"\n",
        "                # Check if the parsed result is a Tree object before attempting to draw\n",
        "                if isinstance(chunk_tree, Tree):\n",
        "                    # Create a TreeView object and save to PostScript\n",
        "                    # Note: This might require ghostscript to be installed in the environment\n",
        "                    # and still might not display directly as an image inline.\n",
        "                    # It saves a .ps file that you would need to download and view.\n",
        "                    from nltk.draw.tree import TreeView\n",
        "                    TreeView(chunk_tree).save(output_filename)\n",
        "                    print(f\"Graphical tree saved to {output_filename}\")\n",
        "                else:\n",
        "                    print(f\"Skipping graphical save for Entry {i+1} as the parsed result is not a Tree object.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Could not generate graphical tree for Entry {i+1}: {e}\")\n",
        "                print(\"Graphical visualization in Colab can be challenging.\")\n",
        "\n",
        "        else:\n",
        "            print(f\"\\n--- Entry {i+1} ---\")\n",
        "            print(\"No POS tags found for this entry.\")\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame is empty or 'pos_tags' column not found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csa7pl6SrVt9"
      },
      "source": [
        "EXP 7 Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpCgpHt-rcWT",
        "outputId": "0e948474-0896-478f-f7db-47d665b7da99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text named_entities\n",
            "0  Subject: enron methanol ; meter # : 988291\\r\\n...             []\n",
            "1  Subject: hpl nom for january 9 , 2001\\r\\n( see...             []\n",
            "2  Subject: neon retreat\\r\\nho ho ho , we ' re ar...             []\n",
            "3  Subject: photoshop , windows , office . cheap ...             []\n",
            "4  Subject: re : indian springs\\r\\nthis deal is t...             []\n",
            "5  Subject: ehronline web address change\\r\\nthis ...             []\n",
            "6  Subject: spring savings certificate - take 30 ...             []\n",
            "7  Subject: looking for medication ? we ` re the ...             []\n",
            "8  Subject: noms / actual flow for 2 / 26\\r\\nwe a...             []\n",
            "9  Subject: nominations for oct . 21 - 23 , 2000\\...             []\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.chunk import ne_chunk # Import ne_chunk\n",
        "from nltk.tree import Tree\n",
        "import os # Import os for file operations\n",
        "\n",
        "# Ensure the required NLTK data for NER is downloaded\n",
        "try:\n",
        "    nltk.data.find('chunkers/maxent_ne_chunker_tab/english_ace_multiclass/')\n",
        "except LookupError:\n",
        "    nltk.download('maxent_ne_chunker_tab')\n",
        "\n",
        "# Download the 'words' corpus if not already present\n",
        "try:\n",
        "    nltk.data.find('corpora/words')\n",
        "except LookupError:\n",
        "    nltk.download('words')\n",
        "\n",
        "\n",
        "# Apply NER only on first 10 rows for speed\n",
        "sample_df = df.head(10).copy()\n",
        "\n",
        "def extract_named_entities(pos_tags):\n",
        "    tree = ne_chunk(pos_tags)\n",
        "    entities = []\n",
        "    for subtree in tree:\n",
        "        if isinstance(subtree, Tree):\n",
        "            entity_name = \" \".join([token for token, pos in subtree.leaves()])\n",
        "            entity_type = subtree.label()\n",
        "            entities.append((entity_name, entity_type))\n",
        "    return entities\n",
        "\n",
        "sample_df['named_entities'] = sample_df['pos_tags'].apply(extract_named_entities)\n",
        "\n",
        "print(sample_df[['text', 'named_entities']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDpyOHtqrhyH"
      },
      "source": [
        "EXP 8: WordNet & Brown Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5huQbX-rk6-",
        "outputId": "01562a4d-dfa0-4d1b-e3f1-ca91c22af3fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'word': 'win', 'synonyms': ['acquire', 'advance', 'winnings', 'profits', 'win'], 'antonyms': ['losings', 'fall_back', 'lose', 'fail'], 'definition': 'a victory (as in a race or other competition)'}\n",
            "----------------------------------------\n",
            "{'word': 'claim', 'synonyms': ['lay_claim', 'arrogate', 'exact', 'title', 'claim'], 'antonyms': ['forfeit', 'disclaim'], 'definition': 'an assertion of a right (as to money or property)'}\n",
            "----------------------------------------\n",
            "{'word': 'money', 'synonyms': ['money'], 'antonyms': [], 'definition': 'the most common medium of exchange; functions as legal tender'}\n",
            "----------------------------------------\n",
            "{'word': 'free', 'synonyms': ['unblock', 'liberate', 'gratis', 'give_up', 'exempt'], 'antonyms': ['lodge', 'enforce', 'obstruct', 'freeze', 'bound'], 'definition': 'people who are free'}\n",
            "----------------------------------------\n",
            "{'word': 'offer', 'synonyms': ['put_up', 'declare_oneself', 'bid', 'propose', 'volunteer'], 'antonyms': [], 'definition': 'the verbal act of offering'}\n",
            "----------------------------------------\n",
            "\n",
            "Sample from Brown Corpus (news category):\n",
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet as wn, brown\n",
        "nltk.download('wordnet')\n",
        "nltk.download('brown')\n",
        "\n",
        "def get_wordnet_info(word):\n",
        "    synsets = wn.synsets(word)\n",
        "    if not synsets:\n",
        "        return None\n",
        "\n",
        "    synonyms = set()\n",
        "    antonyms = set()\n",
        "    for syn in synsets:\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "            if lemma.antonyms():\n",
        "                antonyms.add(lemma.antonyms()[0].name())\n",
        "\n",
        "    return {\n",
        "        \"word\": word,\n",
        "        \"synonyms\": list(synonyms)[:5],\n",
        "        \"antonyms\": list(antonyms)[:5],\n",
        "        \"definition\": synsets[0].definition()\n",
        "    }\n",
        "\n",
        "sample_words = ['win', 'claim', 'money', 'free', 'offer']\n",
        "\n",
        "for w in sample_words:\n",
        "    print(get_wordnet_info(w))\n",
        "    print(\"-\"*40)\n",
        "\n",
        "# Display Brown Corpus sample\n",
        "print(\"\\nSample from Brown Corpus (news category):\")\n",
        "print(brown.words(categories='news')[:20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_Jaxoy5rqlO"
      },
      "source": [
        "EXP 9: Word2Vec + Word Sense Disambiguation (WSD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIKwJfEnxob7",
        "outputId": "39d25e17-2924-4092-d8fc-1bd1f6ff58a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  \\\n",
            "0  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
            "1  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
            "2  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
            "3  Subject: photoshop , windows , office . cheap ...   \n",
            "4  Subject: re : indian springs\\r\\nthis deal is t...   \n",
            "\n",
            "                                    lemmatized_words  \\\n",
            "0  [subject, :, enron, methanol, ;, meter, #, :, ...   \n",
            "1  [subject, :, hpl, nom, for, january, 9, ,, 200...   \n",
            "2  [subject, :, neon, retreat, ho, ho, ho, ,, we,...   \n",
            "3  [subject, :, photoshop, ,, window, ,, office, ...   \n",
            "4  [subject, :, re, :, indian, spring, this, deal...   \n",
            "\n",
            "                                          wsd_senses  \n",
            "0  [subject.n.06, :, enron, methanol.n.01, ;, met...  \n",
            "1  [submit.v.01, :, hpl, nom, for, january.n.01, ...  \n",
            "2  [subject.n.05, :, neon.n.01, retreat.v.04, hol...  \n",
            "3  [subject.n.05, :, photoshop, ,, windowpane.n.0...  \n",
            "4  [subject.n.06, :, ra.n.02, :, indian.a.01, spr...  \n"
          ]
        }
      ],
      "source": [
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import wordnet as wn\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer # Import the lemmatizer\n",
        "from nltk.tokenize import word_tokenize # Import word_tokenize\n",
        "\n",
        "# Reload the original dataframe as it was overwritten in a previous cell\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/spam_ham_dataset.csv\", encoding='latin-1')\n",
        "\n",
        "# Re-apply tokenization and lemmatization to the reloaded DataFrame\n",
        "df['cleaned_nltk_word_tokens'] = df['text'].apply(lambda x: word_tokenize(str(x).lower()))\n",
        "lemmatizer = WordNetLemmatizer() # Initialize lemmatizer\n",
        "df['lemmatized_words'] = df['cleaned_nltk_word_tokens'].apply(lambda tokens: [lemmatizer.lemmatize(t) for t in tokens])\n",
        "\n",
        "\n",
        "# Example function to get the correct WordNet sense\n",
        "def get_wsd_sense(sentence_tokens):\n",
        "    senses = []\n",
        "    for word in sentence_tokens:\n",
        "        synset = lesk(sentence_tokens, word)\n",
        "        if synset:\n",
        "            senses.append(synset.name())  # word sense identifier\n",
        "        else:\n",
        "            senses.append(word)  # fallback to original word\n",
        "    return senses\n",
        "\n",
        "# Apply WSD on your dataset (example for first 5 rows)\n",
        "df['wsd_senses'] = df['lemmatized_words'].apply(get_wsd_sense)\n",
        "\n",
        "print(df[['text', 'lemmatized_words', 'wsd_senses']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHtPIvmxuVWZ"
      },
      "outputs": [],
      "source": [
        "# Combine lemmatized words back into a single string for each email\n",
        "df['processed_text'] = df['lemmatized_words'].apply(lambda x: \" \".join(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VclmfFlxuclF"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # You can tune max_features\n",
        "X = vectorizer.fit_transform(df['processed_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD5QpDEJuerQ"
      },
      "outputs": [],
      "source": [
        "df['label'] = df['label_num'] if 'label_num' in df.columns else (df['label'].map({'spam':1, 'ham':0}))\n",
        "y = df['label']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snRxcESwunMp",
        "outputId": "541572be-5406-4900-8b60-8982ff54d769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model trained successfully!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(df['processed_text'])\n",
        "y = df['label']  # 0 = ham, 1 = spam\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train classifier\n",
        "clf = LogisticRegression(max_iter=500)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"‚úÖ Model trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BZ051o2upXv",
        "outputId": "7d895407-319e-4c8a-9b00-c07c53a64f2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9835748792270531\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       735\n",
            "           1       0.96      0.98      0.97       300\n",
            "\n",
            "    accuracy                           0.98      1035\n",
            "   macro avg       0.98      0.98      0.98      1035\n",
            "weighted avg       0.98      0.98      0.98      1035\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[724  11]\n",
            " [  6 294]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgHZeq_gus3S",
        "outputId": "2eed805b-f49e-459d-bec3-08b10e567375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model & vectorizer saved locally!\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Save classifier\n",
        "with open(\"/content/spam_classifier.pkl\", \"wb\") as f:\n",
        "    pickle.dump(clf, f)\n",
        "\n",
        "# Save vectorizer\n",
        "with open(\"/content/tfidf_vectorizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "print(\"‚úÖ Model & vectorizer saved locally!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "S-YqUwtavsA4",
        "outputId": "e0770b36-3133-46ee-a74a-223663c899ad"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_968d0721-fe4b-45c3-88ae-5b9b35163548\", \"spam_classifier.pkl\", 40721)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_1f6173bc-79dd-4757-b29f-71bbc81d8fc2\", \"tfidf_vectorizer.pkl\", 188470)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/spam_classifier.pkl\")\n",
        "files.download(\"/content/tfidf_vectorizer.pkl\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
